(this["webpackJsonpsmart-bottle"]=this["webpackJsonpsmart-bottle"]||[]).push([[0],{16:function(e,t,a){},19:function(e,t,a){"use strict";a.r(t);var i=a(0),n=a(1),s=a.n(n),o=a(5),r=a.n(o),c=a(7),d=a(8),l=a(10),h=a(9),m=a(2),b=(a.p,a.p+"static/media/bottle.f690dc39.jpg"),p=a.p+"static/media/cad.305d571d.png",f=a.p+"static/media/camera.317e429a.png",j=a.p+"static/media/lid_only.13d57b29.png",u=a.p+"static/media/whole_bottle.0a8f5267.png",g=a.p+"static/media/candy.a8b01988.png",x=a.p+"static/media/inertial+legend.a54f5ed9.png",O=a.p+"static/media/sam.26e7921d.png",w=a.p+"static/media/overlap_shortest.deeb89a0.png",y=(a.p,a.p+"static/media/integreation1.470686d8.png"),v=a.p+"static/media/integreation2.93fc708e.png",N=(a(16),function(e){Object(l.a)(a,e);var t=Object(h.a)(a);function a(){return Object(c.a)(this,a),t.apply(this,arguments)}return Object(d.a)(a,[{key:"render",value:function(){return Object(i.jsx)("div",{className:"background",children:Object(i.jsxs)("div",{style:{display:"flex",alignItems:"center",flexWrap:"wrap"},children:[Object(i.jsxs)("div",{className:"my-container",style:{marginTop:"150px"},children:[Object(i.jsx)("p",{className:"title",children:"Multimodal Sensing for Tracking Medication Adherence"}),Object(i.jsx)("p",{className:"sub-title",children:"Final Term Project for EE 382V - Activity Sensing and Recognition"}),Object(i.jsx)("p",{className:"sub-title",style:{fontStyle:"italic"},children:"Corey Karnei, Connor Fritz"})]}),Object(i.jsxs)("div",{className:"my-container",children:[Object(i.jsx)("p",{className:"sub-heading",children:"Motivation"}),Object(i.jsxs)("p",{className:"txt",children:["Estimates from the World Health Organization indicate that patients in developed countries only take about 50% of prescribed medicines for chronic diseases like hypertension and diabetes. Although seniors are the largest consumers of healthcare resources, studies indicate that as many as 55% of them do not properly take their medications and up to 30% of all hospital readmissions are due to medication non-adherence ",Object(i.jsx)("a",{href:"#ref1",id:"ref1b",children:"[1]"}),". Not only is this non-adherence damaging the health of the individuals it affects, in many cases it can be life threatening. It\u2019s reported that as many as 125,000 people die every year due to failure to take their prescription medication ",Object(i.jsx)("a",{href:"#ref2",id:"ref2b",children:"[2]"}),". Additionally, in patients that suffer from chronic mental illness medication non-adherence rates can be as high as 40-50%. Unlike senior citizens, medication non-adherence in chronically mentally ill patients is often conscious and not due to forgetfulness. Medication non-adherence in these patients can lead to hospitalizations, self-harm, violence or suicide ",Object(i.jsx)("a",{href:"#ref3",id:"ref3b",children:"[3]"}),"."]}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"Because medication non-adherence can cause financial and social harm, it would be beneficial for both physicians and patients to be able to track and detect instances of medication adherence, and by extension, non-adherence. Furthermore, a system for detecting medication adherence would have to be both mobile and robust for instances in which a patient tries to \u2018trick\u2019 the system. Our project aims to provide a proof-of-concept in which both camera and inertial data is used to distinguish pill taking behaviors. Future works could extend this system to obscure the cameras and inertial sensors while also taking steps to eliminate privacy concerns - thus providing a mobile system that is more difficult to \u2018trick\u2019 than the smart pill bottles currently on the market."})]}),Object(i.jsxs)("div",{className:"my-container",children:[Object(i.jsx)("p",{className:"sub-heading",children:" Prior Work "}),Object(i.jsxs)("p",{className:"txt",children:["A significant body of research has been conducted to improve adherence to prescription medications through various interventions. Several researchers have attempted to detect pill-taking gestures using wrist-mounted inertial sensors ",Object(i.jsx)("a",{href:"#ref4",id:"ref4b",children:"[4]"}),Object(i.jsx)("a",{href:"#ref5",id:"ref5b",children:"[5]"}),". One group equipped a pill bottle with an inertial sensor to classify activities ",Object(i.jsx)("a",{href:"#ref6",id:"ref6b",children:"[6]"}),". Others have embedded pill bottle lids with sensors that detect whether the container is open or not ",Object(i.jsx)("a",{href:"#ref7",id:"ref7b",children:"[7]"}),Object(i.jsx)("a",{href:"#ref8",id:"ref8b",children:"[8]"}),". One approach involved RFID chips on pill bottles to detect if the bottle is taken from a medicine cabinet combined with face detection and tracking from a stationary camera ",Object(i.jsx)("a",{href:"#ref9",id:"ref9b",children:"[9]"}),". Many of these papers tried to increase adherence by reminding the user when they fail to take their medication. We aimed to develop a system that could be applicable for senior citizens as well as mentally ill patients who don\u2019t take their medicine by choice rather than by accident. We believe that our approach is novel due to the combination of inertial and video sensing, and because the cameras are able to move with the bottle. More specifically, we could not find other projects that applied hand and face tracking to data from mobile cameras mounted on the bottle."]}),Object(i.jsx)(m.a,{src:b,className:"img",rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 2. Existing Smart Bottle Prototype "})]}),Object(i.jsxs)("div",{className:"my-container",children:[Object(i.jsx)("p",{className:"sub-heading",children:"Procedure"}),Object(i.jsx)("p",{className:"txt",children:"Our plan was to build a system for tracking medication intake behavior using a combination of inertial sensing and computer vision. To this end, we employed an inertial sensor to classify what action was being performed to the pill bottle, an internal camera that would serve to indicate whether the bottle cap had been removed, and external cameras that would recognize when a pill-taking motion had happened. In combination, these would be able to give a physician a good sense as to whether a patient had taken a pill or not."}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"To mount the internal and external cameras to the pill bottle, we 3-d modeled and printed a plastic piece that would attach to the pill bottle\u2019s cap, and could house 4 cameras facing in each direction."}),Object(i.jsx)(m.a,{src:p,className:"img",rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 2. 3D Model for Camera Mount "}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"We employed several miniature cameras purchased online to record video while the pill bottle was in use. Each miniature camera cost about $15 and was attached to the 3D printed mount with a velcro strip."}),Object(i.jsx)(m.a,{src:f,className:"img",rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 3. Miniature Camera "}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"We also used a cell phone running the app sensorlog to capture inertial data, which was attached to the bottom of the bottle. The internal camera was affixed to the underside of the bottle cap."}),Object(i.jsx)(m.a,{src:j,className:"img",rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 4. Internal Camera Fixation "}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"The final design is shown below, with the cameras affixed to the mount and the phone attached to the bottom of the assembly."}),Object(i.jsx)(m.a,{src:u,className:"img",rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 5. Complete Design "}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("hr",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"A total of 5 particpants were recruited to gather data for this project. Each participant was presented with a script that detailed specific actions for the participants to make sequentially. They were instructed to repeat the steps of the script 10 times, taking one pill each time for a total of 50 pill-taking moments. All participants completed their tasks on the same day and in the same location. Each person was also filmed with a smartphone while they performed the tasks for the purpose of gathering the 'true labels' for each model."})]}),Object(i.jsxs)("div",{className:"my-container",children:[Object(i.jsx)("p",{className:"sub-heading",children:" Results "}),Object(i.jsx)("p",{className:"txt",children:"As previously stated, three different models were employed; one for the internal camera, one for the inertial sensor, and one for the external cameras. Each will be described in more detail here."}),Object(i.jsx)("br",{}),Object(i.jsx)("hr",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"sub-sub-heading",children:" Internal Camera - Model 1 "}),Object(i.jsx)("p",{className:"txt",children:"To detect whether the the pill bottle cap had been removed, we trained a simple convolutional neural network (CNN) on the footage from the internal camera. Within the CNN, the RELU activtion function was used. All images from the camera were scaled to a 64x64 pixel size and were converted to gradyscale to increase the speed and decrease the memory consumption of the CNN. Additionally, the resizing of the images mutated the time stamps at the bottom left corner of each image, thereby minimizing the chance that these could be erroneosly used as features by the CNN. Image augmentations were also perfomed via brightening/darkening and vertcial flipping to increase the size of the data set and the accuracy of the model. Shown below is an example image from the internal camera."}),Object(i.jsx)(m.a,{src:g,className:"img",rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 6. Internal Camera View When Bottle Closed "}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"This method achieved a 91% leave one participant out cross validation (LOPOCV) accuracy on just the raw image set, and 94% LOPOCV when the data set was expanded by 100% with augmented images. However, using a CNN to detect whether or not the bottle is opened or closed would not be the best method to employ in a consumer product. A switch would have a near 100% accuracy while also being smaller, cheaper, and less battery intensive than a machine learning approach. However, applying a CNN to this particular problem taught us more about image manipulation and the image labelling process."}),Object(i.jsx)("br",{}),Object(i.jsx)("hr",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"sub-sub-heading",children:" Inertial Sensor - Model 2 "}),Object(i.jsx)("p",{className:"txt",children:"For the inertial sensor, we extracted 4 features from each dimension of movement : mean, variance, skew, and kurtosis. We then used a random forest classifier to determine what activity was being performed. Below, you can see a snippet of the data which contains 4 pill-taking moments. The true labels were created manually after watching the smartphone video recorded separately. In each pill-taking gesture, you can see it go from stationary to picking up to getting pill to putting down."}),Object(i.jsx)(m.a,{src:x,className:"img",rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 7. Inertial Sensor Data "}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"We reached 95% accuracy when trained on a random split, and 85% when using LOPOCV. These results are reasonable, and since most pill bottles spend most of the time in one spot, it is very unlikely that an entire pill taking moment would be completely missed by this model."}),Object(i.jsx)("br",{}),Object(i.jsx)("hr",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"sub-sub-heading",children:" External Cameras - Model 3 "}),Object(i.jsxs)("p",{className:"txt",children:["To detect when a pill has been taken, we made use of an existing system, YOLOv4 ",Object(i.jsx)("a",{href:"#ref10",id:"ref10b",children:"[10]"}),". You only look once version 4 (YOLOv4) is a state-of-the-art, real-time object detection framework. YOLOv4 is able to detect many different objects at once, and will provide bounding boxes wherever it detects each object. The default version of this tool is trained on dogs, cats, horses, planes, and 16 other classes. But since none of these were hands or faces, we sought out databases for these classes individually and trained our own versions of YOLOv4 to detect them both. We used the Oxford Hands Dataset ",Object(i.jsx)("a",{href:"#ref11",id:"ref11b",children:"[11]"})," and the WIDER Faces Dataset ",Object(i.jsx)("a",{href:"#ref12",id:"ref12b",children:"[12]"}),". While a single YOLOv4 model is able to detect multiple objects, we trained two separate models - one for faces and one for hands. This is because many of the pictures in the Oxford Hands Dataset contain un-annotated faces, and many of the pictures in the WIDER Faces Dataset contain un-annotated hands. If these datasets were combined to train a single YOLOv4 model, the model would have a difficult time learning defining features with so many unlabelled hands and faces present."]}),Object(i.jsx)(m.a,{src:O,className:"img",rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 8. External Camera Model Bounding Boxes "}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"After training our two separate YOLOv4 models, we ran the external camera videos through our models to generate face and hand bounding boxes. Pill taking instances were recorded when two criteria were met in the same frame. A hand and a face bounding box had to overlap, and the area of the hand bounding box had to be smaller than the area of the face bounding box. The first criterion is meant to detect when a participant has their hand near their face. The second criterion is meant to cull false positives caused when a participant's hand obscures their face from the camera's view. Pill taking instances within 1 second of one another are then grouped together to form pill taking moments. Below you can see a snippet of one pill-taking moment and its composite pill taking instances. On the top row in red are the frames where our model detected a hand. On the bottom in blue are the frames where our model detected a face. In the middle in green are the frames where both criteria are met. The black X indicates the true label of the pill taking moment."}),Object(i.jsx)(m.a,{src:w,className:"img",rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 9. A Pill Taking Moment "}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"Neither LOPOCV nor a train/test split were used to evaulate this model as the model was not trained on the external camera data. Instead, the participant's data was use to test the complete model. Overall, 80% of the 50 true pill taking moments were detected. There were also very few false positives. Some of the pill taking moments were only partially in frame and we believe this caused the accuracy to be lower than it could have been otherwise. In a real implementation, this could be remedied by using a fish eye lense camera, or by using more cameras to expand the device's field of view."})]}),Object(i.jsxs)("div",{className:"my-container",children:[Object(i.jsx)("p",{className:"sub-heading",children:" Integration "}),Object(i.jsx)("p",{className:"txt",children:"Since this project was motivated by the desire to allow physicians to know how well their patients are taking their medicine, we have proposed a system for integrating these separate models. When given data for a period of time, say a day, each model would perform its evaluation independently and return a boolean indicating whether or not that model believes a pill was taken. These booleans could then be summed and sent to the patient\u2019s doctor, acting as a score denoting how confident the system is that a pill was taken."}),Object(i.jsx)(m.a,{src:y,className:"img",rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 10. Potential Model Integration "}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"In such a system, the doctor would be able to look at a dashboard like the one below."}),Object(i.jsx)(m.a,{src:v,className:"img",style:{width:"700px"},rounded:!0}),Object(i.jsx)("p",{className:"fig-description",children:"Fig 11. Dashboard Mockup"}),Object(i.jsx)("br",{}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"Across time, the doctor would be able to get a decent understanding of how well the patient has been adhering to their medication and could intervene if they see results like those shown in the bottom figure. Combining the predictions in this way makes the system more robust, as even if one of the models is wrong on a given day, across all 3 models and across multiple days the trend should still be clear."})]}),Object(i.jsxs)("div",{className:"my-container",children:[Object(i.jsx)("p",{className:"sub-heading",children:" Limitations and Future Work "}),Object(i.jsx)("p",{className:"txt",children:"This work had several limiatations, most of which would be easily addressed in future work on this topic."}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"The first is the disconnectedness of the sensors. Each sensor was its own device and the fact that each one started collecting data at a slightly different time made it much more difficult to try and actually implement an integrated system like the one described in the previous section. Additionally, having to consolidate the data from 6 different devices made scaling up our data collection somewhat impractical. In the future, we would consolidate the sensors to one device, perhaps using a microcontroller, which would allow us to activate all the sensors at once and store the data in one location."}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"The second limitation was the limited field of view. Although we positioned the external cameras in all four directions, there was still a few degrees between any two cameras that was a blind spot. This led to errors in our object detection models, as some pill taking moments were not visible in the camera data. A fisheye lens could possibly be employed to solve this issue."}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"A final limitation pertains to privacy. Two of our models used cameras to capture data, but cameras come with a litany of privacy concerns that our work did nothing to address. To remedy this, an infrared camera could be used instead of standard video cameras. Additionally, in a consumer-ready product, the internal camera could be replaced by a switch of some sort. Additionally, hand and face detection could be run in real time on the microcontroller, and the camera data could be discarded."}),Object(i.jsx)("br",{}),Object(i.jsx)("p",{className:"txt",children:"Overall, if we were to extend this work further, we would create a system that consolidates the models' output into a adherence confidence score, flesh out and develop the propsed physician portal, and address the limitations listed above."})]}),Object(i.jsxs)("div",{className:"my-container",children:[Object(i.jsx)("p",{className:"sub-heading",children:" Conclusion "}),Object(i.jsx)("p",{className:"txt",children:"In this report we described a novel, multimodal approach for tracking medication adherence. Three separate models were created to detect aspects of pill taking gestures. The first model classified whether the bottle was open or closed based off of data from an internal camera. This model was able to achieve a 94% LOPOCV accuracy. The second model used inertial data collected from a smartphone attached to the bottle. This model classified whether the bottle was stationary, being picked up, being put down, or being moved by the participant to obtain a pill. This model was able to achieve an 85% LOPOCV accuracy. The third and final model used data collected from four external cameras mounted to the pill bottle. This model used two separate YOLOv4 instances trained on outside datasets - one for hand detection, and one for face detection. This model combined the outputs from the YOLOv4 instances to detect pill taking moments with an 80% accuracy when tested on the totality of the external camera data. To our knowledge, this is the first project to use both inertial sensing and camera data collected from a mobile bottle to detect pill taking gestures. In the future, this project could be expanded by connecting the sensors, integrating the models, addressing privacy concerns, and creating a user friendly app for physicians to track patient pill taking behavior."})]}),Object(i.jsxs)("div",{className:"my-container",style:{marginBottom:"0px"},children:[Object(i.jsx)("p",{className:"sub-heading",children:" References "}),Object(i.jsxs)("p",{className:"reference",id:"ref1",children:[Object(i.jsx)("a",{href:"#ref1b",children:"[1]"}),' \u201cMedipense " Top 10 Reasons Seniors Do Not Take Their Medications.\u201d Medipense, 9 May 2018, medipense.com/en/top-10-reasons-seniors-do-not-take-their-medications/.']}),Object(i.jsxs)("p",{className:"reference",id:"ref2",children:[Object(i.jsx)("a",{href:"#ref2b",children:"[2]"})," Benjamin, Regina M. \u201cMedication Adherence: Helping Patients Take Their Medicines as Directed.\u201d Public Health Reports, vol. 127, no. 1, 2012, pp. 2\u20133., doi:10.1177/003335491212700102."]}),Object(i.jsxs)("p",{className:"reference",id:"ref3",children:[Object(i.jsx)("a",{href:"#ref3b",children:"[3]"})," Acar, Gulsah, et al. \u201cMedication Non-Adherence in Chronic Mental Illness: Management Strategies\u201d ARC Journal of Psychiatry, vol. 2, no. 1, 2017, pp. 23-25."]}),Object(i.jsxs)("p",{className:"reference",id:"ref4",children:[Object(i.jsx)("a",{href:"#ref4b",children:"[4]"})," Marquard, Jenna L, et al. \u201cDesigning a Wrist-Worn Sensor to Improve Medication Adherence: Accommodating Diverse User Behaviors and Technology Preferences.\u201d JAMIA Open, vol. 1, no. 2, 2018, pp. 153\u2013158., doi:10.1093/jamiaopen/ooy035."]}),Object(i.jsxs)("p",{className:"reference",id:"ref5",children:[Object(i.jsx)("a",{href:"#ref5b",children:"[5]"})," Kalantarian, Haik, et al. \u201cA Smartwatch-Based Medication Adherence System.\u201d 2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN), 2015, doi:10.1109/bsn.2015.7299348."]}),Object(i.jsxs)("p",{className:"reference",id:"ref6",children:[Object(i.jsx)("a",{href:"#ref6b",children:"[6]"})," Aldeer, Murtadha, et al. \u201cA Sensing-Based Framework for Medication Compliance Monitoring.\u201d Proceedings of the 1st ACM International Workshop on Device-Free Human Sensing - DFHS'19, 2019, doi:10.1145/3360773.3360886."]}),Object(i.jsxs)("p",{className:"reference",id:"ref7",children:[Object(i.jsx)("a",{href:"#ref7b",children:"[7]"})," Reese, Peter P., et al. \u201cAutomated Reminders and Physician Notification to Promote Immunosuppression Adherence Among Kidney Transplant Recipients: A Randomized Trial.\u201d American Journal of Kidney Diseases, W.B. Saunders, 7 Dec. 2016, www.sciencedirect.com/science/article/pii/S0272638616305972."]}),Object(i.jsxs)("p",{className:"reference",id:"ref8",children:[Object(i.jsx)("a",{href:"#ref8b",children:"[8]"})," Shellmer, Diana A., and Nataliya Zelikovsky. \u201cThe Challenges of Using Medication Event Monitoring Technology with Pediatric Transplant Patients.\u201d Pediatric Transplantation, vol. 11, no. 4, 2007, pp. 422\u2013428., doi:10.1111/j.1399-3046.2007.00681.x."]}),Object(i.jsxs)("p",{className:"reference",id:"ref9",children:[Object(i.jsx)("a",{href:"#ref9b",children:"[9]"})," Hasanuzzaman, Faiz M., et al. \u201cMonitoring Activity of Taking Medicine by Incorporating RFID and Video Analysis.\u201d Network Modeling Analysis in Health Informatics and Bioinformatics, vol. 2, no. 2, 2013, pp. 61\u201370., doi:10.1007/s13721-013-0025-y."]}),Object(i.jsxs)("p",{className:"reference",id:"ref10",children:[Object(i.jsx)("a",{href:"#ref10b",children:"[10]"}),' Bochkovskiy, Alexey, et al. "YOLOv4: Optimal Speed and Accuracy of Object Detection." arXiv preprint arXiv:2004.10934v1, 2020']}),Object(i.jsxs)("p",{className:"reference",id:"ref11",children:[Object(i.jsx)("a",{href:"#ref11b",children:"[11]"}),' Mittal, A., et al. "Hand Detection Using Multiple Proposals" British Machine Vision Conference, 2011']}),Object(i.jsxs)("p",{className:"reference",id:"ref12",children:[Object(i.jsx)("a",{href:"#ref12b",children:"[12]"}),' Yang, Shuo, et al. "WIDER FACE: A Face Detection Benchmark" IEEE Conference on Computer Vision and Pattern Recognition, 2016']})]})]})})}}]),a}(n.Component));r.a.render(Object(i.jsx)(s.a.StrictMode,{children:Object(i.jsx)(N,{})}),document.getElementById("root"))}},[[19,1,2]]]);
//# sourceMappingURL=main.d0fc6609.chunk.js.map