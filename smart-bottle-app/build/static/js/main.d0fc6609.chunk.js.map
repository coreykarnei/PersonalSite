{"version":3,"sources":["imgs/bottle.jpg","imgs/test.jpg","imgs/cad.png","imgs/camera.png","imgs/lid_only.png","imgs/whole_bottle.png","imgs/candy.png","imgs/inertial+legend.png","imgs/sam.png","imgs/overlap_shortest.png","imgs/integreation1.png","imgs/conf.png","imgs/integreation2.png","App.js","index.js"],"names":["App","className","style","display","alignItems","flexWrap","marginTop","fontStyle","href","id","Image","src","glow_bottle_img","rounded","cad_img","camera_img","lid_img","whole_bottle_img","candy_img","inertial_img","sam_img","overlap_img","integration1_img","integration2_img","width","marginBottom","Component","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"8NAAe,GCAA,IDAA,IAA0B,oCEA1B,MAA0B,gCCA1B,MAA0B,mCCA1B,MAA0B,qCCA1B,MAA0B,yCCA1B,MAA0B,kCCA1B,MAA0B,4CCA1B,MAA0B,gCCA1B,MAA0B,6CCA1B,GCAA,IDAA,IAA0B,2CEA1B,MAA0B,0CCsc1BA,G,6KA3aX,OACE,qBAAKC,UAAU,aAAf,SACI,sBAAKC,MAAO,CAACC,QAAQ,OAAQC,WAAW,SAAUC,SAAS,QAA3D,UACE,sBAAKJ,UAAU,eAAeC,MAAO,CAACI,UAAU,SAAhD,UACE,mBAAGL,UAAU,QAAb,kEAGA,mBAAGA,UAAU,YAAb,+EAGA,mBAAGA,UAAU,YAAYC,MAAO,CAACK,UAAU,UAA3C,2CAKJ,sBAAKN,UAAU,eAAf,UACE,mBAAGA,UAAU,cAAb,wBACA,oBAAGA,UAAU,MAAb,gbAKc,mBAAGO,KAAK,QAAQC,GAAG,QAAnB,iBALd,uPAO8C,mBAAGD,KAAK,QAAQC,GAAG,QAAnB,iBAP9C,+WAUwG,mBAAGD,KAAK,QAAQC,GAAG,QAAnB,iBAVxG,OAYA,uBACA,uBACA,mBAAGR,UAAU,MAAb,wyBAYF,sBAAKA,UAAU,eAAf,UACE,mBAAGA,UAAU,cAAb,0BACA,oBAAGA,UAAU,MAAb,sPAG8C,mBAAGO,KAAK,QAAQC,GAAG,QAAnB,iBAAkC,mBAAGD,KAAK,QAAQC,GAAG,QAAnB,iBAHhF,qFAI8B,mBAAGD,KAAK,QAAQC,GAAG,QAAnB,iBAJ9B,yGAKyB,mBAAGD,KAAK,QAAQC,GAAG,QAAnB,iBAAkC,mBAAGD,KAAK,QAAQC,GAAG,QAAnB,iBAL3D,kLAMiG,mBAAGD,KAAK,QAAQC,GAAG,QAAnB,iBANjG,ykBAeA,cAACC,EAAA,EAAD,CAAOC,IAAKC,EAAiBX,UAAU,MAAMY,SAAO,IACpD,mBAAGZ,UAAU,kBAAb,wDAIF,sBAAKA,UAAU,eAAf,UACE,mBAAGA,UAAU,cAAb,uBACA,mBAAGA,UAAU,MAAb,8hBAQA,uBACA,uBACA,mBAAGA,UAAU,MAAb,4NAKA,cAACS,EAAA,EAAD,CAAOC,IAAKG,EAASb,UAAU,MAAMY,SAAO,IAC5C,mBAAGZ,UAAU,kBAAb,+CACA,uBACA,uBAEA,mBAAGA,UAAU,MAAb,yNAMA,cAACS,EAAA,EAAD,CAAOC,IAAKI,EAAYd,UAAU,MAAMY,SAAO,IAC/C,mBAAGZ,UAAU,kBAAb,sCACA,uBACA,uBAEA,mBAAGA,UAAU,MAAb,+MAKA,cAACS,EAAA,EAAD,CAAOC,IAAKK,EAASf,UAAU,MAAMY,SAAO,IAC5C,mBAAGZ,UAAU,kBAAb,8CACA,uBACA,uBAEA,mBAAGA,UAAU,MAAb,0IAIA,cAACS,EAAA,EAAD,CAAOC,IAAKM,EAAkBhB,UAAU,MAAMY,SAAO,IACrD,mBAAGZ,UAAU,kBAAb,qCAEA,uBACA,uBACA,uBACA,uBAEA,mBAAGA,UAAU,MAAb,+iBASF,sBAAKA,UAAU,eAAf,UACE,mBAAGA,UAAU,cAAb,uBACA,mBAAGA,UAAU,MAAb,iNAIA,uBACA,uBACA,uBACA,mBAAGA,UAAU,kBAAb,yCACA,mBAAGA,UAAU,MAAb,4xBAcA,cAACS,EAAA,EAAD,CAAOC,IAAKO,EAAWjB,UAAU,MAAMY,SAAO,IAC9C,mBAAGZ,UAAU,kBAAb,6DACA,uBACA,uBACA,mBAAGA,UAAU,MAAb,6lBASA,uBACA,uBACA,uBACA,mBAAGA,UAAU,kBAAb,yCACA,mBAAGA,UAAU,MAAb,wfASA,cAACS,EAAA,EAAD,CAAOC,IAAKQ,EAAclB,UAAU,MAAMY,SAAO,IACjD,mBAAGZ,UAAU,kBAAb,0CACA,uBACA,uBACA,mBAAGA,UAAU,MAAb,8RAOA,uBACA,uBACA,uBACA,mBAAGA,UAAU,kBAAb,0CACA,oBAAGA,UAAU,MAAb,6FAEiB,mBAAGO,KAAK,SAASC,GAAG,SAApB,kBAFjB,ogBAS+E,mBAAGD,KAAK,SAASC,GAAG,SAApB,kBAT/E,gCAUsB,mBAAGD,KAAK,SAASC,GAAG,SAApB,kBAVtB,weAmBA,cAACC,EAAA,EAAD,CAAOC,IAAKS,EAASnB,UAAU,MAAMY,SAAO,IAC5C,mBAAGZ,UAAU,kBAAb,0DACA,uBACA,uBACA,mBAAGA,UAAU,MAAb,mjCAoBA,cAACS,EAAA,EAAD,CAAOC,IAAKU,EAAapB,UAAU,MAAMY,SAAO,IAChD,mBAAGZ,UAAU,kBAAb,0CACA,uBACA,uBACA,mBAAGA,UAAU,MAAb,omBAYF,sBAAKA,UAAU,eAAf,UACE,mBAAGA,UAAU,cAAb,2BACA,mBAAGA,UAAU,MAAb,iiBAQA,cAACS,EAAA,EAAD,CAAOC,IAAKW,EAAkBrB,UAAU,MAAMY,SAAO,IACrD,mBAAGZ,UAAU,kBAAb,kDACA,uBACA,uBACA,mBAAGA,UAAU,MAAb,mGAGA,cAACS,EAAA,EAAD,CAAOC,IAAKY,EAAkBtB,UAAU,MAAMC,MAAO,CAACsB,MAAM,SAAUX,SAAO,IAC7E,mBAAGZ,UAAU,kBAAb,sCACA,uBACA,uBACA,mBAAGA,UAAU,MAAb,yaAQF,sBAAKA,UAAU,eAAf,UACE,mBAAGA,UAAU,cAAb,2CACA,mBAAGA,UAAU,MAAb,uHAIE,uBACA,mBAAGA,UAAU,MAAb,0mBASA,uBACA,mBAAGA,UAAU,MAAb,sYAOA,uBACA,mBAAGA,UAAU,MAAb,6fAUA,uBACA,mBAAGA,UAAU,MAAb,gQASJ,sBAAKA,UAAU,eAAf,UACE,mBAAGA,UAAU,cAAb,0BACA,mBAAGA,UAAU,MAAb,i3CAuBF,sBAAKA,UAAU,eAAeC,MAAO,CAACuB,aAAa,OAAnD,UACE,mBAAGxB,UAAU,cAAb,0BACA,oBAAGA,UAAU,YAAYQ,GAAG,OAA5B,UACE,mBAAGD,KAAK,SAAR,iBADF,mLAIA,oBAAGP,UAAU,YAAYQ,GAAG,OAA5B,UACE,mBAAGD,KAAK,SAAR,iBADF,4MAKA,oBAAGP,UAAU,YAAYQ,GAAG,OAA5B,UACE,mBAAGD,KAAK,SAAR,iBADF,4KAIA,oBAAGP,UAAU,YAAYQ,GAAG,OAA5B,UACE,mBAAGD,KAAK,SAAR,iBADF,8PAMA,oBAAGP,UAAU,YAAYQ,GAAG,OAA5B,UACE,mBAAGD,KAAK,SAAR,iBADF,gOAMA,oBAAGP,UAAU,YAAYQ,GAAG,OAA5B,UACE,mBAAGD,KAAK,SAAR,iBADF,wOAMA,oBAAGP,UAAU,YAAYQ,GAAG,OAA5B,UACE,mBAAGD,KAAK,SAAR,iBADF,sTAOA,oBAAGP,UAAU,YAAYQ,GAAG,OAA5B,UACE,mBAAGD,KAAK,SAAR,iBADF,4QAMA,oBAAGP,UAAU,YAAYQ,GAAG,OAA5B,UACE,mBAAGD,KAAK,SAAR,iBADF,0QAMA,oBAAGP,UAAU,YAAYQ,GAAG,QAA5B,UACE,mBAAGD,KAAK,UAAR,kBADF,oIAKA,oBAAGP,UAAU,YAAYQ,GAAG,QAA5B,UACE,mBAAGD,KAAK,UAAR,kBADF,2GAKA,oBAAGP,UAAU,YAAYQ,GAAG,QAA5B,UACE,mBAAGD,KAAK,UAAR,kBADF,+I,GAlaMkB,cCnBlBC,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,W","file":"static/js/main.d0fc6609.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/bottle.f690dc39.jpg\";","export default __webpack_public_path__ + \"static/media/test.ac17b149.jpg\";","export default __webpack_public_path__ + \"static/media/cad.305d571d.png\";","export default __webpack_public_path__ + \"static/media/camera.317e429a.png\";","export default __webpack_public_path__ + \"static/media/lid_only.13d57b29.png\";","export default __webpack_public_path__ + \"static/media/whole_bottle.0a8f5267.png\";","export default __webpack_public_path__ + \"static/media/candy.a8b01988.png\";","export default __webpack_public_path__ + \"static/media/inertial+legend.a54f5ed9.png\";","export default __webpack_public_path__ + \"static/media/sam.26e7921d.png\";","export default __webpack_public_path__ + \"static/media/overlap_shortest.deeb89a0.png\";","export default __webpack_public_path__ + \"static/media/integreation1.470686d8.png\";","export default __webpack_public_path__ + \"static/media/conf.72c02d5a.png\";","export default __webpack_public_path__ + \"static/media/integreation2.93fc708e.png\";","\nimport React, {Component} from 'react';\nimport ReactDOM from 'react-dom';\n\nimport Image from 'react-bootstrap/Image';\n\nimport test_img from './imgs/test.jpg';\nimport glow_bottle_img from './imgs/bottle.jpg';\nimport cad_img from './imgs/cad.png';\nimport camera_img from './imgs/camera.png';\nimport lid_img from './imgs/lid_only.png';\nimport whole_bottle_img from './imgs/whole_bottle.png';\nimport candy_img from './imgs/candy.png';\nimport inertial_img from './imgs/inertial+legend.png';\nimport sam_img from './imgs/sam.png';\nimport yolo_img from './imgs/yolo.png';\nimport overlap_img from './imgs/overlap_shortest.png';\nimport conf_img from './imgs/conf.png';\nimport integration1_img from './imgs/integreation1.png';\nimport integration2_img from './imgs/integreation2.png';\n\nimport './css/App.css';\n\nclass App extends Component{\n\n  render(){\n\n    return (\n      <div className=\"background\">\n          <div style={{display:\"flex\", alignItems:\"center\", flexWrap:\"wrap\"}}>\n            <div className=\"my-container\" style={{marginTop:\"150px\"}}>\n              <p className=\"title\">\n                Multimodal Sensing for Tracking Medication Adherence\n              </p>\n              <p className=\"sub-title\">\n                Final Term Project for EE 382V - Activity Sensing and Recognition\n              </p>\n              <p className=\"sub-title\" style={{fontStyle:'italic'}}>\n                Corey Karnei, Connor Fritz\n              </p>\n            </div>\n\n          <div className=\"my-container\">\n            <p className=\"sub-heading\">Motivation</p>\n            <p className=\"txt\">\n            Estimates from the World Health Organization indicate that patients in developed countries only\n            take about 50% of prescribed medicines for chronic diseases like hypertension and diabetes. Although\n            seniors are the largest consumers of healthcare resources, studies indicate that as many as 55% of them do\n            not properly take their medications and up to 30% of all hospital readmissions are due to medication\n            non-adherence <a href=\"#ref1\" id=\"ref1b\">[1]</a>. Not only is this non-adherence damaging the health of the individuals it affects, in\n            many cases it can be life threatening. It’s reported that as many as 125,000 people die every year due to\n            failure to take their prescription medication <a href=\"#ref2\" id=\"ref2b\">[2]</a>. Additionally, in patients that suffer from chronic mental\n            illness medication non-adherence rates can be as high as 40-50%. Unlike senior citizens, medication\n            non-adherence in chronically mentally ill patients is often conscious and not due to forgetfulness.\n            Medication non-adherence in these patients can lead to hospitalizations, self-harm, violence or suicide <a href=\"#ref3\" id=\"ref3b\">[3]</a>.\n            </p>\n            <br></br>\n            <br></br>\n            <p className=\"txt\">\n            Because medication non-adherence can cause financial and social harm, it would be beneficial for\n            both physicians and patients to be able to track and detect instances of medication adherence, and by\n            extension, non-adherence. Furthermore, a system for detecting medication adherence would have to be\n            both mobile and robust for instances in which a patient tries to ‘trick’ the system. Our project aims to\n            provide a proof-of-concept in which both camera and inertial data is used to distinguish pill taking\n            behaviors. Future works could extend this system to obscure the cameras and inertial sensors while also\n            taking steps to eliminate privacy concerns - thus providing a mobile system that is more difficult to ‘trick’\n            than the smart pill bottles currently on the market.\n            </p>\n          </div>\n\n          <div className=\"my-container\">\n            <p className=\"sub-heading\"> Prior Work </p>\n            <p className=\"txt\">\n            A significant body of research has been conducted to improve adherence to prescription\n            medications through various interventions. Several researchers have attempted to detect pill-taking\n            gestures using wrist-mounted inertial sensors <a href=\"#ref4\" id=\"ref4b\">[4]</a><a href=\"#ref5\" id=\"ref5b\">[5]</a>. One group equipped a pill bottle with an inertial\n            sensor to classify activities <a href=\"#ref6\" id=\"ref6b\">[6]</a>. Others have embedded pill bottle lids with sensors that detect whether the\n            container is open or not <a href=\"#ref7\" id=\"ref7b\">[7]</a><a href=\"#ref8\" id=\"ref8b\">[8]</a>. One approach involved RFID chips on pill bottles to detect if the bottle is\n            taken from a medicine cabinet combined with face detection and tracking from a stationary camera <a href=\"#ref9\" id=\"ref9b\">[9]</a>.\n            Many of these papers tried to increase adherence by reminding the user when they fail to take their\n            medication. We aimed to develop a system that could be applicable for senior citizens as well as mentally ill\n            patients who don’t take their medicine by choice rather than by accident. We believe that our approach is\n            novel due to the combination of inertial and video sensing, and because the cameras are able to move with\n            the bottle. More specifically, we could not find other projects that applied hand and face\n            tracking to data from mobile cameras mounted on the bottle.\n            </p>\n\n            <Image src={glow_bottle_img} className=\"img\" rounded />\n            <p className=\"fig-description\">Fig 2. Existing Smart Bottle Prototype </p>\n\n          </div>\n\n          <div className=\"my-container\">\n            <p className=\"sub-heading\">Procedure</p>\n            <p className=\"txt\">\n            Our plan was to build a system for tracking medication intake behavior using a combination of inertial\n            sensing and computer vision. To this end, we employed an inertial sensor to classify what action was\n            being performed to the pill bottle, an internal camera that would serve to indicate\n            whether the bottle cap had been removed, and external cameras that would recognize when\n            a pill-taking motion had happened. In combination, these would be able to give a\n            physician a good sense as to whether a patient had taken a pill or not.\n            </p>\n            <br></br>\n            <br></br>\n            <p className=\"txt\">\n            To mount the internal and external cameras to the pill bottle, we\n            3-d modeled and printed a plastic piece that would attach to the pill bottle’s cap, and could\n            house 4 cameras facing in each direction.\n            </p>\n            <Image src={cad_img} className=\"img\" rounded />\n            <p className=\"fig-description\">Fig 2. 3D Model for Camera Mount </p>\n            <br></br>\n            <br></br>\n\n            <p className=\"txt\">\n            We employed several miniature\n            cameras purchased online to record video while the pill bottle\n            was in use. Each miniature camera cost about $15 and was attached\n            to the 3D printed mount with a velcro strip.\n            </p>\n            <Image src={camera_img} className=\"img\" rounded />\n            <p className=\"fig-description\">Fig 3. Miniature Camera </p>\n            <br></br>\n            <br></br>\n\n            <p className=\"txt\">\n            We also used a cell phone\n            running the app sensorlog to capture inertial data, which was attached to the bottom\n            of the bottle. The internal camera was affixed to the underside of the bottle cap.\n            </p>\n            <Image src={lid_img} className=\"img\" rounded />\n            <p className=\"fig-description\">Fig 4. Internal Camera Fixation </p>\n            <br></br>\n            <br></br>\n\n            <p className=\"txt\">\n            The final design is shown below, with the cameras affixed to the mount and the phone attached\n            to the bottom of the assembly.\n            </p>\n            <Image src={whole_bottle_img} className=\"img\" rounded />\n            <p className=\"fig-description\">Fig 5. Complete Design </p>\n\n            <br></br>\n            <br></br>\n            <hr></hr>\n            <br></br>\n\n            <p className=\"txt\">\n            A total of 5 particpants were recruited to gather data for this project. Each participant was presented\n            with a script that detailed specific actions for the participants to make sequentially. They were instructed to\n            repeat the steps of the script 10 times, taking one pill each time for a total of 50 pill-taking moments. All participants\n            completed their tasks on the same day and in the same location. Each person was also filmed with a smartphone while they\n            performed the tasks for the purpose of gathering the 'true labels' for each model.\n            </p>\n          </div>\n\n          <div className=\"my-container\">\n            <p className=\"sub-heading\"> Results </p>\n            <p className=\"txt\">\n              As previously stated, three different models were employed; one for the internal camera, one for the inertial sensor,\n              and one for the external cameras. Each will be described in more detail here.\n            </p>\n            <br></br>\n            <hr></hr>\n            <br></br>\n            <p className=\"sub-sub-heading\"> Internal Camera - Model 1 </p>\n            <p className=\"txt\">\n              To detect whether the the pill bottle cap had been removed, we\n              trained a simple convolutional neural network (CNN) on the footage from the\n              internal camera. Within the CNN, the RELU activtion function was used. All\n              images from the camera were scaled to a 64x64 pixel size and were\n              converted to gradyscale to increase the speed and decrease the memory consumption\n              of the CNN. Additionally, the resizing of the images mutated the time\n              stamps at the bottom left corner of each image, thereby minimizing\n              the chance that these could be erroneosly used as features by\n              the CNN. Image augmentations were\n              also perfomed via brightening/darkening and vertcial flipping to increase the size of the\n              data set and the accuracy of the model. Shown below is an example image from the internal\n              camera.\n            </p>\n            <Image src={candy_img} className=\"img\" rounded />\n            <p className=\"fig-description\">Fig 6. Internal Camera View When Bottle Closed </p>\n            <br />\n            <br />\n            <p className=\"txt\">\n            This method achieved a 91% leave one participant out cross validation (LOPOCV) accuracy on just the\n            raw image set, and 94% LOPOCV when the data set was expanded by 100% with augmented images. However, using\n            a CNN to detect whether or not the bottle is opened or closed would not be\n            the best method to employ in a consumer product. A switch would have a near 100% accuracy\n            while also being smaller, cheaper, and less battery intensive than a machine learning\n            approach. However, applying a CNN to this particular problem\n            taught us more about image manipulation and the image labelling process.\n            </p>\n            <br></br>\n            <hr></hr>\n            <br></br>\n            <p className=\"sub-sub-heading\"> Inertial Sensor - Model 2 </p>\n            <p className=\"txt\">\n            For the inertial sensor, we extracted 4 features from each dimension of movement :\n            mean, variance, skew, and kurtosis. We then used a random forest classifier\n            to determine what activity was being performed. Below, you can\n            see a snippet of the data which contains 4 pill-taking moments. The true labels\n            were created manually after watching the smartphone video recorded separately.\n            In each pill-taking gesture, you can see it go from stationary to picking up to getting pill\n            to putting down.\n            </p>\n            <Image src={inertial_img} className=\"img\" rounded />\n            <p className=\"fig-description\">Fig 7. Inertial Sensor Data </p>\n            <br />\n            <br />\n            <p className=\"txt\">\n            We reached 95% accuracy when trained on a random split,\n            and 85% when using LOPOCV. These results are reasonable,\n            and since most pill bottles spend most of the time in one spot, it is\n            very unlikely that an entire pill taking moment would be completely\n             missed by this model.\n            </p>\n            <br></br>\n            <hr></hr>\n            <br></br>\n            <p className=\"sub-sub-heading\"> External Cameras - Model 3 </p>\n            <p className=\"txt\">\n              To detect when a pill has been taken, we made use of an existing\n              system, YOLOv4 <a href=\"#ref10\" id=\"ref10b\">[10]</a>.\n              You only look once version 4 (YOLOv4) is a state-of-the-art, real-time\n              object detection framework. YOLOv4 is able to detect many different\n              objects at once, and will provide bounding boxes wherever it\n              detects each object. The default version of this tool is trained on dogs, cats,\n              horses, planes, and 16 other classes. But since none of these were hands\n              or faces, we sought out databases for these classes individually and trained our\n              own versions of YOLOv4 to detect them both. We used the Oxford Hands Dataset <a href=\"#ref11\" id=\"ref11b\">[11]</a> and the\n              WIDER Faces Dataset <a href=\"#ref12\" id=\"ref12b\">[12]</a>. While a single YOLOv4 model is able to detect multiple objects,\n              we trained two separate models - one for faces and one for hands.\n              This is because many of the pictures in the Oxford Hands Dataset contain\n              un-annotated faces, and many of the pictures in the WIDER Faces Dataset\n              contain un-annotated hands.\n              If these datasets were combined to train a single YOLOv4 model,\n              the model would have a difficult time learning defining features\n              with so many unlabelled hands and faces present.\n            </p>\n            <Image src={sam_img} className=\"img\" rounded />\n            <p className=\"fig-description\">Fig 8. External Camera Model Bounding Boxes </p>\n            <br />\n            <br />\n            <p className=\"txt\">\n            After training our two separate YOLOv4 models, we ran the external\n            camera videos through our models to generate face and hand bounding boxes.\n            Pill taking instances were recorded when two\n            criteria were met in the same frame. A hand and a face bounding box\n            had to overlap, and the area of the hand bounding box had to be smaller\n            than the area of the face bounding box. The first criterion is meant to\n            detect when a participant has their hand near their face. The second\n            criterion is meant to cull false positives caused when a\n            participant's hand obscures their face from the camera's view.\n            Pill taking instances within 1 second of one another are then grouped\n            together to form pill taking moments.\n\n            Below you can see a snippet of one pill-taking moment\n            and its composite pill taking instances. On the top row in red are\n            the frames where our model detected a hand. On the bottom in blue are\n            the frames where our model detected a face. In the middle\n            in green are the frames where both criteria are met. The\n            black X indicates the true label of the pill taking moment.\n            </p>\n            <Image src={overlap_img} className=\"img\" rounded />\n            <p className=\"fig-description\">Fig 9. A Pill Taking Moment </p>\n            <br />\n            <br />\n            <p className=\"txt\">\n            Neither LOPOCV nor a train/test split\n            were used to evaulate this model as the model was not trained on the external camera data.\n            Instead, the participant's data was use to test the complete model.\n            Overall, 80% of the 50 true pill taking moments were detected. There\n            were also very few false positives. Some of the pill taking moments were only partially in frame\n            and we believe this caused the accuracy to be lower than it could have been otherwise.\n            In a real implementation, this could be remedied by using a fish eye lense camera,\n            or by using more cameras to expand the device's field of view.\n            </p>\n          </div>\n\n          <div className=\"my-container\">\n            <p className=\"sub-heading\"> Integration </p>\n            <p className=\"txt\">\n            Since this project was motivated by the desire to allow physicians to know how well their\n            patients are taking their medicine, we have proposed a system for integrating these separate\n            models. When given data for a period of time, say a day, each model would perform its evaluation\n            independently and return a boolean indicating whether or not that model believes a pill was\n            taken. These booleans could then be summed and sent to the patient’s doctor, acting as a score\n            denoting how confident the system is that a pill was taken.\n            </p>\n            <Image src={integration1_img} className=\"img\" rounded />\n            <p className=\"fig-description\">Fig 10. Potential Model Integration </p>\n            <br />\n            <br />\n            <p className=\"txt\">\n            In such a system, the doctor would be able to look at a dashboard like the one below.\n            </p>\n            <Image src={integration2_img} className=\"img\" style={{width:\"700px\"}} rounded />\n            <p className=\"fig-description\">Fig 11. Dashboard Mockup</p>\n            <br />\n            <br />\n            <p className=\"txt\">\n            Across time, the doctor would be able to get a decent understanding of how well the\n            patient has been adhering to their medication and could intervene if they see results like\n            those shown in the bottom figure. Combining the predictions in this way makes the system more robust, as\n            even if one of the models is wrong on a given day, across all 3 models and across multiple days\n            the trend should still be clear.\n            </p>\n          </div>\n          <div className=\"my-container\">\n            <p className=\"sub-heading\"> Limitations and Future Work </p>\n            <p className=\"txt\">\n              This work had several limiatations, most of which would be easily addressed in\n              future work on this topic.\n              </p>\n              <br></br>\n              <p className=\"txt\">\n              The first is the disconnectedness of the sensors. Each sensor was its own device\n              and the fact that each one started collecting data at a slightly different time made it much more difficult to\n              try and actually implement an integrated system like the one described in the previous section.\n              Additionally, having to consolidate\n              the data from 6 different devices made scaling up our data collection somewhat impractical. In the future,\n              we would consolidate the sensors to one device, perhaps using a microcontroller, which would allow us to activate\n              all the sensors at once and store the data in one location.\n              </p>\n              <br></br>\n              <p className=\"txt\">\n              The second limitation was the limited field of view.\n              Although we positioned the external cameras in all four directions, there was still a few degrees between any two cameras that\n              was a blind spot. This led to errors in our object detection models,\n              as some pill taking moments were not visible in the camera data.\n              A fisheye lens could possibly be employed to solve this issue.\n              </p>\n              <br></br>\n              <p className=\"txt\">\n              A final limitation pertains to privacy. Two of\n              our models used cameras to capture data,\n              but cameras come with a litany of privacy concerns that our work did nothing to address. To remedy this,\n              an infrared camera could be used instead of standard video cameras.\n              Additionally, in a consumer-ready product, the internal camera could\n              be replaced by a switch of some sort. Additionally, hand and face\n              detection could be run in real time on the microcontroller, and the\n              camera data could be discarded.\n              </p>\n              <br></br>\n              <p className=\"txt\">\n              Overall, if we were to extend this work further, we would create a\n              system that consolidates\n              the models' output into a adherence confidence score, flesh out\n              and develop the propsed physician portal, and address the limitations\n              listed above.\n            </p>\n          </div>\n\n          <div className=\"my-container\">\n            <p className=\"sub-heading\"> Conclusion </p>\n            <p className=\"txt\">\n              In this report we described a novel, multimodal approach for tracking medication adherence.\n              Three separate models were created to detect aspects of pill taking gestures.\n              The first model classified whether the bottle was open or closed based\n              off of data from an internal camera. This model was able to achieve\n              a 94% LOPOCV accuracy. The second model used inertial data collected\n              from a smartphone attached to the bottle. This model classified whether\n              the bottle was stationary, being picked up, being put down, or being\n              moved by the participant to obtain a pill. This model was able to achieve\n              an 85% LOPOCV accuracy. The third and final model used data collected\n              from four external cameras mounted to the pill bottle. This model used\n              two separate YOLOv4 instances trained on outside datasets - one for hand detection, and one for\n              face detection. This model combined the outputs from the YOLOv4 instances\n              to detect pill taking moments with an 80% accuracy when tested on\n              the totality of the external camera data. To our knowledge, this\n              is the first project to use both inertial sensing and camera data\n              collected from a mobile bottle to detect pill taking gestures. In\n              the future, this project could be expanded by connecting the sensors,\n              integrating the models, addressing privacy concerns, and creating a\n              user friendly app for physicians to track patient pill taking behavior.\n            </p>\n          </div>\n\n          <div className=\"my-container\" style={{marginBottom:\"0px\"}}>\n            <p className=\"sub-heading\"> References </p>\n            <p className=\"reference\" id=\"ref1\">\n              <a href=\"#ref1b\">[1]</a> “Medipense \" Top 10 Reasons Seniors Do Not Take Their Medications.” Medipense, 9 May\n                  2018, medipense.com/en/top-10-reasons-seniors-do-not-take-their-medications/.\n            </p>\n            <p className=\"reference\" id=\"ref2\">\n              <a href=\"#ref2b\">[2]</a> Benjamin, Regina M. “Medication Adherence: Helping Patients Take Their Medicines as\n                  Directed.” Public Health Reports, vol. 127, no. 1, 2012, pp. 2–3.,\n                  doi:10.1177/003335491212700102.\n            </p>\n            <p className=\"reference\" id=\"ref3\">\n              <a href=\"#ref3b\">[3]</a> Acar, Gulsah, et al. “Medication Non-Adherence in Chronic Mental Illness: Management\n                  Strategies” ARC Journal of Psychiatry, vol. 2, no. 1, 2017, pp. 23-25.\n            </p>\n            <p className=\"reference\" id=\"ref4\">\n              <a href=\"#ref4b\">[4]</a> Marquard, Jenna L, et al. “Designing a Wrist-Worn Sensor to Improve Medication Adherence:\n              Accommodating Diverse User Behaviors and Technology Preferences.” JAMIA Open, vol. 1, no.\n              2, 2018, pp. 153–158., doi:10.1093/jamiaopen/ooy035.\n            </p>\n\n            <p className=\"reference\" id=\"ref5\">\n              <a href=\"#ref5b\">[5]</a> Kalantarian, Haik, et al. “A Smartwatch-Based Medication Adherence System.” 2015 IEEE 12th\n              International Conference on Wearable and Implantable Body Sensor Networks (BSN), 2015,\n              doi:10.1109/bsn.2015.7299348.\n            </p>\n\n            <p className=\"reference\" id=\"ref6\">\n              <a href=\"#ref6b\">[6]</a> Aldeer, Murtadha, et al. “A Sensing-Based Framework for Medication Compliance Monitoring.”\n              Proceedings of the 1st ACM International Workshop on Device-Free Human Sensing - DFHS'19,\n              2019, doi:10.1145/3360773.3360886.\n            </p>\n\n            <p className=\"reference\" id=\"ref7\">\n              <a href=\"#ref7b\">[7]</a> Reese, Peter P., et al. “Automated Reminders and Physician Notification to Promote\n              Immunosuppression Adherence Among Kidney Transplant Recipients: A Randomized Trial.”\n              American Journal of Kidney Diseases, W.B. Saunders, 7 Dec. 2016,\n              www.sciencedirect.com/science/article/pii/S0272638616305972.\n            </p>\n\n            <p className=\"reference\" id=\"ref8\">\n              <a href=\"#ref8b\">[8]</a> Shellmer, Diana A., and Nataliya Zelikovsky. “The Challenges of Using Medication Event\n              Monitoring Technology with Pediatric Transplant Patients.” Pediatric Transplantation, vol. 11,\n              no. 4, 2007, pp. 422–428., doi:10.1111/j.1399-3046.2007.00681.x.\n            </p>\n\n            <p className=\"reference\" id=\"ref9\">\n              <a href=\"#ref9b\">[9]</a> Hasanuzzaman, Faiz M., et al. “Monitoring Activity of Taking Medicine by Incorporating RFID\n              and Video Analysis.” Network Modeling Analysis in Health Informatics and Bioinformatics, vol.\n              2, no. 2, 2013, pp. 61–70., doi:10.1007/s13721-013-0025-y.\n            </p>\n\n            <p className=\"reference\" id=\"ref10\">\n              <a href=\"#ref10b\">[10]</a> Bochkovskiy, Alexey, et al. \"YOLOv4: Optimal Speed and Accuracy of Object Detection.\"\n              arXiv preprint arXiv:2004.10934v1, 2020\n            </p>\n\n            <p className=\"reference\" id=\"ref11\">\n              <a href=\"#ref11b\">[11]</a> Mittal, A., et al. \"Hand Detection Using Multiple Proposals\"\n              British Machine Vision Conference, 2011\n            </p>\n\n            <p className=\"reference\" id=\"ref12\">\n              <a href=\"#ref12b\">[12]</a> Yang, Shuo, et al. \"WIDER FACE: A Face Detection Benchmark\"\n              IEEE Conference on Computer Vision and Pattern Recognition, 2016\n            </p>\n\n          </div>\n\n        </div>\n      </div>\n    );\n  }\n}\n\nexport default App;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport App from './App';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n"],"sourceRoot":""}